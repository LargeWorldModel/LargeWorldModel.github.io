<!DOCTYPE html>
<html class="fontawesome-i2svg-active fontawesome-i2svg-complete">
<head>
    <title>Large World Models</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="description" content="">
    <meta name="keywords" content="Large World Models, LWM">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
    <script src="./static/css/index.js"></script>
  </head>
  <body>

    <nav class="navbar" role="navigation" aria-label="main navigation">
      <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
        </a>
      </div>
    </nav>


    <section class="hero">
      <div class="hero-body">
        <div class="container is-widescreen">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">World Model on Million-Length Video and <br> Language with RingAttention</h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="https://www.haoliu.site/">Hao Liu</a><sup>*</sup>,</span>&nbsp;
                <span class="author-block">
                  <a href="https://wilson1yan.github.io/">Wilson Yan</a><sup>*</sup>,</span>&nbsp;
                <span class="author-block">
                  <a href="https://people.eecs.berkeley.edu/~matei/">Matei Zaharia</a>,&nbsp;
                </span>
                <span class="author-block">
                  <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>&nbsp;
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block">UC berkeley </span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="TODO" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="ai ai-arxiv"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://github.com/LargeWorldModel/lwm" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg>
                      </span>
                      <span>Code</span>
                      </a>
                  </span>
                  <span class="link-block">
                    <a href="https://huggingface.co/LargeWorldModel" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <svg class="svg-inline--fa fa-images fa-w-18" aria-hidden="true" focusable="false" data-prefix="far" data-icon="images" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512" data-fa-i2svg=""><path fill="currentColor" d="M480 416v16c0 26.51-21.49 48-48 48H48c-26.51 0-48-21.49-48-48V176c0-26.51 21.49-48 48-48h16v48H54a6 6 0 0 0-6 6v244a6 6 0 0 0 6 6h372a6 6 0 0 0 6-6v-10h48zm42-336H150a6 6 0 0 0-6 6v244a6 6 0 0 0 6 6h372a6 6 0 0 0 6-6V86a6 6 0 0 0-6-6zm6-48c26.51 0 48 21.49 48 48v256c0 26.51-21.49 48-48 48H144c-26.51 0-48-21.49-48-48V80c0-26.51 21.49-48 48-48h384zM264 144c0 22.091-17.909 40-40 40s-40-17.909-40-40 17.909-40 40-40 40 17.909 40 40zm-72 96l39.515-39.515c4.686-4.686 12.284-4.686 16.971 0L288 240l103.515-103.515c4.686-4.686 12.284-4.686 16.971 0L480 208v80H192v-48z"></path></svg>
                      </span>
                    <span>Model</span>
                    </a>
                  </span>
                </div>

              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Current language models fall short in understanding aspects of the world not easily described in words, and struggle with complex, long-form tasks. Video sequences offer valuable temporal information absent in language and static images, making them attractive for joint modeling with language. Such models could develop a understanding of both human textual knowledge and the physical world, enabling broader AI capabilities for assisting humans. However, learning from millions of tokens in video and language sequences poses challenges due to memory constraints, computational complexity, and limited datasets. We address these challenges with RingAttention, a technique for scaling context size arbitrarily without approximations or overheads, enabling efficient training on long sequences. We curate a large dataset of diverse videos and long-form books, and gradually increase context size from 32K to 1M tokens during training to manage computational costs. This paper makes the following contributions: (a) Largest context size neural network: We train one of the largest context size transformers on video and text sequences, setting new benchmarks in difficult retrieval tasks and comprehensive long video understanding. (b) Solutions for overcoming video-text training challenges, including using variable batch sizes for different sequence types and leveraging text data for regularization during video training. (c) A highly-optimized implementation of RingAttention, variable batches and other key features for large context transformer training. (d) A fully open-sourced 7B parameter model capable of processing over 1M tokens.
                This work paves the way for training on massive datasets of long video and language, enabling the development of AI systems with a grounded understanding of the world and broader capabilities.
                <br><br><br><br>
              </p>
            </div>
          </div>
        </div>
        <!--/ Abstract. -->

        <!-- Data figure. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3" style="white-space: nowrap;">Autoregressive Prediction on Diverse Videos and Books.</h2>
            <div class="content has-text-justified">
              <div style="text-align: center;">
              <img src="./materials/data_fig.png" alt="Data Mixture">
              <p class="caption" style="width: 100%; text-align: center;"><b>Figure 1. Context Extension and Vision-Language Training.</b> Expanding context size from 4K to 1M on books, followed by vision-language training on diverse forms of visual contents of varying lengths. The lower panel shows interactive capabilities in understanding and responding to queries about complex multimodal world.</p>
            </div>
            </div>
            <br><br>
          </div>
        </div>
        <!-- / Data figure. -->

        <!-- Model figure. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3" style="white-space: nowrap;">Any to Any Prediction with Autoregressive RingAttention.</h2>
            <div class="content has-text-justified">
              <div style="text-align: center;">
              <img src="./materials/model.png" alt="Model" width="600" height="500">
              <p class="caption" style="width: 100%; text-align: center;"><b>Figure 2. Any-to-Any Autoregressive Prediction.</b> RingAttention allows us to use a very large context window for training on diverse formats of video-text, text-video, image-text, text-image, pure video, pure image, and pure text.</p>
              </div>
            </div>
            <br><br>
          </div>
        </div>
        <!-- / Model figure. -->

        <!-- Fact figure. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3" style="white-space: nowrap;">Large and Effective 1M Context.</h2>
            <div class="content has-text-justified">
              <div style="text-align: center;">
              <img src="./materials/single_needle_1M.png" alt="Fact retrieval" width="700">
              </div>
            </div>
            <div class="content has-text-justified">
              <div style="text-align: center;">
              <img src="./materials/multi_needle.png" alt="Fact retrieval" width="700">
              <p class="caption" style="width: 100%; text-align: center;"><b>Figure 3. Needle retrieval tasks.</b> LWM achieves high accuracy across 1M context size window.</p>
              </div>
            </div>
            <br><br>
          </div>
        </div>
        <!-- / Fact figure. -->

      </div>
  </section>
